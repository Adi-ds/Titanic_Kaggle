Consider a dataset with features <img src="https://render.githubusercontent.com/render/math?math=\Large x_1, x_2, ....., x_n, y">.

The hypothesis function h is defined as <img src="https://render.githubusercontent.com/render/math?math=\Large h(x) = g(w^Tx %2B b)">, where <img src="https://render.githubusercontent.com/render/math?math=\Large X = (x_1, x_2, ...., x_n), w = (w_1, w_2, ...., w_n)"> and <img src="https://render.githubusercontent.com/render/math?math=\Large g(z) = \lbrace \begin{matrix} +1, z \geq 0 \\ -1, otherwise \end{matrix}"> . This “w, b” notation allows to explicitly treat the intercept term b separately from the other parameters.

Note that, from the definition of <img src="https://render.githubusercontent.com/render/math?math=\Large g"> above, the classifier will directly predict either +1 or −1, i.e, the point above or on the hyperplane will be classified as class +1, and the point below the hyperplane will be classified as class -1.

Given a training example <img src="https://render.githubusercontent.com/render/math?math=\Large (x_i , y_i)">, the functional margin of (w, b) can be defined as
<img src="https://render.githubusercontent.com/render/math?math=\Large h(x_i) = y_i(w^Tx_i %2B b)">, where <img src = "https://render.githubusercontent.com/render/math?math=\Large y_i = -1, +1"> and <img src = "https://render.githubusercontent.com/render/math?math=\Large x_i = (x_{1j}, x_{2j}, ....., x_{nj})">.

Note that if <img src="https://render.githubusercontent.com/render/math?math=\Large y_i = 1">, then for the functional margin to be large (i.e., for the prediction to be confident and correct), <img src="https://render.githubusercontent.com/render/math?math=\Large w^Tx_i %2B b"> needs to be a large positive number. Conversely, if <img src="https://render.githubusercontent.com/render/math?math=\Large y_i = -1">, then for the functional margin to be large, <img src="https://render.githubusercontent.com/render/math?math=\Large w^Tx_i %2B b"> to be a large negative number. Moreover, if <img src="https://render.githubusercontent.com/render/math?math=\Large y_i(w^Tx_i %2B b) > 0">, then the prediction on this example is correct. Hence, a large functional margin represents a confident and a correct prediction.

For a linear classifier with the choice of <img src="https://render.githubusercontent.com/render/math?math=\Large g"> given above (taking values in {−1, 1}), there’s one property of the functional margin that makes it not a very good measure of confidence, however. Given the choice of <img src="https://render.githubusercontent.com/render/math?math=\Large g">, note that if w is replaced with 2w and b is replaced with 2b, then since <img src="https://render.githubusercontent.com/render/math?math=\Large g(w^Tx %2B b) = g(2w^Tx %2B 2b)">, this would not change <img src="https://render.githubusercontent.com/render/math?math=\Large h_{w,b}(x)"> at all, i.e., <img src="https://render.githubusercontent.com/render/math?math=\Large g">, and hence also <img src="https://render.githubusercontent.com/render/math?math=\Large h_{w,b}(x)">, depends only on the sign, but not on the magnitude of <img src="https://render.githubusercontent.com/render/math?math=\Large w^Tx %2B b">. However, replacing (w, b) with (2w, 2b) also results in multiplying our functional margin by a factor of 2. Thus, it seems that by exploiting the freedom to scale w and b, the functional margin can be made arbitrarily large without really changing anything meaningful. Intuitively, it might be better to impose a normalization condition such as that <img src="https://render.githubusercontent.com/render/math?math=\Large ||w||_2 = 1">; i.e., (w, b) might be replaced with <img src="https://render.githubusercontent.com/render/math?math=\Large (\frac {w}{||w||_2} , \frac {b}{||w||_2})">, and instead consider the functional margin of <img src="https://render.githubusercontent.com/render/math?math=\Large (\frac {w}{||w||_2} , \frac {b}{||w||_2})">.

The above normalization condition can also be visualized geometrically. For this let us consider the 2D case. The distance <img src="https://render.githubusercontent.com/render/math?math=\Large d"> of a point <img src="https://render.githubusercontent.com/render/math?math=\Large (x_0,y_0)">  from a line <img src="https://render.githubusercontent.com/render/math?math=\Large ax %2B by %2B c = 0"> is given by <img src="https://render.githubusercontent.com/render/math?math=\Large d = \frac {|ax_0 %2B by_0 %2B c|}{\sqrt {a^2 %2B b^2}}"> as shown in the following figure.

<p align = "center">
  <img src = "https://editor.analyticsvidhya.com/uploads/70387Distanc.jpg" width = 500 height = 300>
</p>

Simillarly, the distance of a data point <img src="https://render.githubusercontent.com/render/math?math=\Large (x_i,y_i)"> from a hyperplane will be <img src="https://render.githubusercontent.com/render/math?math=\LARGE \frac {|w^Tx_i %2B b|}{||w||_2}"> which gives <img src="https://render.githubusercontent.com/render/math?math=\LARGE  ({\frac {w}{||w||_2}})^Tx_i %2B {\frac {b}{||w||_2}}"> .

Hence, the same normalization condition is obtained.

Note that if <img src="https://render.githubusercontent.com/render/math?math=\Large ||w|| = 1">, then the functional margin equals the geometric margin—this thus gives a way of relating these two different notions of margin. Also, the geometric margin is invariant to rescaling of the parameters, i.e., if w is replaced with 2w and b is replaced with 2b, then the geometric margin does not change.
